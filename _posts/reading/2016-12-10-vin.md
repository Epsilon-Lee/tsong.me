---
layout: post
title: Value Iteration Networks
author: Jiaming Song
tags:
- reading
---

## Value Iteration Networks

### Introduction

The sequential nature of decision making in RL is inherently different than the one-step decisions in supervised learning, and in general requires some form of *planning*. The success of reactive policies in sequential problems is due to the learning algorithm, which essentially trains a reactive policy to select actions with good long-term consequences.

In this work, a NN-based policy is proposed that *learns to plan*. The key is an observation that the classic value iteration (VI) planning algorithm may be represented by a specific type of CNN.

### The Value Iteration Network Model

Let $$M$$ denote the MDP of the domain for which the policy $$\pi$$ is designed. It is assumed that some unknown MDP $$\bar{M}$$ might be useful for learning the optimial policy in the original $$M$$.

Let $$\bar{s}, \bar{a}, \bar{R}, \bar{P}$$ be the states, actions, rewards and transitions in $$\bar{M}$$. $$\bar{R}$$ and $$\bar{P}$$ depend on the observation in $$M$$, namely, $$\bar{R} = f_R(\phi(s))$$ and $$\bar{P} = f_P(\phi(s))$$, and the functions will be learned.

Once an MDP $$\bar{M}$$ has been specified, any standard planning algorithm can be used to obtain the value function $$\bar{V}^\star$$. Two observations:

1. The vector of values $$\hat{V}^\star(s)$$ encodes all the information about the optimal plan in $$\hat{M}$$.
2. The optimal decision $$\hat{\pi}^\star(\hat{s})$$ can depend only on a subset of the values of $$\hat{V}^\star$$. In NN terms, this is a sort of attention.

The attention module learns an attention $$\psi(s)$$, which is used on a reactive policy $$\pi_{rs}(a \lvert \phi(s), \psi(s))$$.

![]({{site.baseurl}}/public/img/reading/vin.png)

### The VI Module

The main observation is that each iteration of VI may be seen as passing the previous value function $$V_n​$$ and reward function $$R​$$ through a convolution layer and max-pooling layer. Each channel corresponds to the $$Q​$$-function for a specific action, and kernel weights correspond to the discounted transition probabilities.

Thus by recurrently applying a convolution layer $$K$$ times, $$K$$ iterations of VI are effectively performed.