---
layout: post
title: Plug & Play Generative Networks
author: Jiaming Song
tags:
- reading
---

## Plug & Play Generative Networks: Conditional Interative Generation of Images in Latent Space

### Iterative Generation of Image Models

The framework generates an image $$x_t$$ through a MCMC sampler whose stationary distribution approximates a given distribution $$p(x)$$. Basically this adopts a stochastic gradient Langevin gradient method:


$$
x_{t+1} = x_t + \epsilon_{12} \nabla \log p(x_t) + \mathcal{N}(0, \epsilon_3^2)
$$



### Activation Maximization

Assume sampling from a joint model $$p(x, y)$$, which can be decomposed into an image model and a classfication model:


$$
p(x, y) = p(x) p(y \lvert x)
$$


We would like to draw from the conditional distribution $$p(x\lvert y=y_c) \propto p(x) p(y = y_c \lvert x)$$.


$$
\begin{array}\
x_{t+1} &= x_t + \epsilon_{12}\nabla p(x_t \lvert y=y_c)  + \mathcal{N}(0, \epsilon_3^2) \\
&= x_t + \epsilon_{12}\nabla \log p(x_t) + \epsilon_{12} \nabla \log p(y = y_c \lvert x_t) + \mathcal{N}(0, \epsilon_3^2) \\
&=x_t + \epsilon_1 \frac{\partial \log p(x_t)}{\partial x_t} + \epsilon_2 \frac{\partial \log p(y = y_c \lvert x_t)}{\partial x_t} + \mathcal{N}(0, \epsilon_3^2)
\end{array}
$$


Decoupling $$\epsilon_1$$ and $$\epsilon_2$$ has the foloowing intuition - 

1. $$\epsilon_1$$ take a step towards a generic image
2. $$\epsilon_2$$ take a step towards an image that causes the classifier to output higher confidence in the chosen class.
3. $$\epsilon_3$$ take a step around to encourage a diversity of images (or basically Langevin dynamics)



### Types of PPGMs

![]({{site.baseurl}}/public/img/reading/ppgn.png)



Basically the different types of PPGMs are different formulation for $$p(x)$$, or joint distribution $$p(x, h)$$.

