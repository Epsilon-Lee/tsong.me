---
layout: post
title: Options Discovery with Budgeted Reinforcement Learning
author: Jiaming Song
tags:
- reading
---

## Options Discovery with Budgeted Reinforcement Learning

### Bi-observations POMDP

In a POMDP, the agent observes $$x_t$$ at each time $$t$$. In Bi-POMDP, in addition to $$x_t$$, the agent can ask for $$y_t$$ which will bring him a supplementary information that will help the agent to decide which action to choose. This is similar to active learning to semi-supervised learning.

Examples include:

- During while using the GPS
- Answering questions while consulting an expert.

### The BONN Architecture

This architecture is very similar to the Hierarchical Multiscale Recurrent Neural Networks

![]({{site.baseurl}}/public/img/reading/bonn.png)



**Option Model**: Choose options $$o_t$$ depending on the additional observations $$y_t$$. 


$$
o_t = f(y_t)
$$




**Actor Model**: Choose action based on the observations $$x_t$$ and the selected option.


$$
z_t = r(o_t, x_t) \quad \mbox{or} \quad z_t = h(z_{t-1}, a_{t-1}, x_t) \\
a_t = u(z_t)
$$




**Aquisition Model:** Determine whether to select a new option or not.


$$
\sigma_t = v(z_{t-1}, a_{t-1}, x_t)
$$




### Budgeted Learning

The BONN model is based on the assumption that the discovery of options will result in learning a good trade off between policy efficiency and the *cognitive effort* generated by such policy.

In the case of BONN, the basic idea is to reduce the number of acquisitions, such that 


$$
r^\star(s_t, a_t, \sigma_t) = r(s_t, a_t) - \lambda \sigma_t
$$
