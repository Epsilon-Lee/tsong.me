<html lang='en'>

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Google's Neural Machine Translation System</title>
    <link href="https://fonts.googleapis.com/css?family=Droid+Sans+Mono" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Noto+Sans" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Roboto+Mono" rel="stylesheet">
    <link href="/public/css/bootstrap.min.css" rel="stylesheet">
    <link href="/public/css/github.css" rel="stylesheet">
    <style>
        hr {
            height: 1px;
        }
        footer {
            font-family: "Noto Sans", "Arial", sans-serif;
            margin-top: 50px;
        }
        img {
            margin-left: auto;
            margin-top: auto;
            margin-bottom: auto;
            margin-right: auto;
            max-width:90%;
            max-height:90%;
        }
        img.center {
            margin-left: auto;
            margin-right: auto;
            padding-bottom: 25px;
            padding-top: 25px;
            max-width:90%;
            max-height:90%;
            display: block;
        }
        p {
            line-height: 1.7; margin-top: 1px; margin-bottom: 1px;
        }
    </style>
</head>

<body style="font-size: 16px;">

<nav class="navbar navbar-default" style="display: block; font-size: 18px;" role="navigation">
    <div class="container col-lg-8 col-lg-offset-2">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-data" aria-expanded="false">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/" style="padding-left: 30px">Jiaming Song</a>
        </div>


        <div id="navbar-data" class="navbar-collapse collapse">
            <ul class="nav navbar-nav">
                <!--li>
                    <a href="/" style="padding-left: 30px">Home</a>
                </li-->
                <li>
                    <a href="/blog" style="padding-left: 30px">Articles</a>
                </li>
                <!--li>
                    <a href="/projects" style="padding-left: 30px">Projects</a>
                </li-->
                <!--li>
                    <a href="/reading" style="padding-left: 30px">Reading</a>
                </li-->
            </ul>
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="https://github.com/jiamings" target="_blank"><img src="/public/img/icon/github.min.svg" height="24px" width="24px"></a>
                </li>
                <li>
                    <a href="https://twitter.com/baaadas" target="_blank"><img src="/public/img/icon/twitter.min.svg" height="24px" width="24px"></a>
                </li>
                <li>
                    <a href="https://cn.linkedin.com/in/jiamings" target="_blank"><img src="/public/img/icon/linkedin.min.svg" height="24px" width="24px"></a>
                </li>
                <li>
                    <a href="https://github.com/jiamings/cv/blob/master/jiaming_cv.pdf" target="_blank">Curriculum Vitae</a>
                </li>
            </ul>
        </div>
    </div>
</nav>


<div class="container" style="margin-top: 50px;">
<div class="row">
<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 col-sm-10 col-sm-offset-1">
      
      
      <h2 id="googles-neural-machine-translation-system">Google’s Neural Machine Translation System</h2>

<p>Neural Machine Translation is an end-to-end approach for automated translation, with the hope of overcoming many of the weaknesses of conventional phrase-based translation systems.</p>

<p>The Phrase-Based Machine Translation (PBMT) methods try to break an input sentence into words and phrases to be translated largely independently, whereas Neural Machine Translation (NMT) considers the entire input sentence as a unit of translation. This paper discusses Google’s recent work on Neural Machine Translation, which utilizes state-of-the-art training techniques to improve the Google Translation system.</p>

<h3 id="architecture">Architecture</h3>

<p>The Google Neural Machine Translation uses Recurrent Neural Networks to directly learn the mapping between sentence in one language to sentence in another language. The overall architecture of the model uses an “encoder - attention - decoder” structure.</p>

<p><img src="/public/img/reading/google-nmt-lstm.png" alt="Image" class="center" /></p>

<h4 id="preliminaries">Preliminaries</h4>

<p>Here are some resources for you to help understand some of the terms.</p>

<ul>
  <li><a href="http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/">Autoencoders</a></li>
  <li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Recurrent Neural Networks and Long Short-Term Memory</a></li>
  <li><a href="http://kaiminghe.com/icml16tutorial/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf">Residual Networks</a></li>
  <li><a href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/">Neural Attention and Memory</a></li>
</ul>

<h4 id="overall-architecture">Overall Architecture</h4>

<p>The overall architecture adopts an “encoder-attention-decoder” architecture.</p>

<ul>
  <li>The encoder module encodes the input sequence of length M to M encoding vectors.</li>
  <li>The decoding module produces a hidden state for the next symbol to be predicted (given the previous symbol and the encoding vectors), which then goes through a softmax layer to generate a probability distribution over candidate output symbols.</li>
  <li>The attention module is responsible for selecting the weights for summation of the encoding vectors at different time steps, such that the encoding vector fed into the decoding module is different at each time step, with the aim of prioritizing attention.</li>
</ul>

<p><img src="/public/img/reading/nmt-model-fast.gif" alt="" class="center" /></p>

<h4 id="encoder">Encoder</h4>

<p>The encoder is an LSTM-RNN of 8 layers with residual connections between layers, but Google did some tweaks to the architecture for better performance (both accuracy and speed).</p>

<h5 id="bidirectional-lstm-in-the-first-layer">Bidirectional LSTM in the First Layer</h5>

<p>The first two layers are bi-directional, which takes both forwards and backwards context into consideration. The only reason why Google did not use bidirectional LSTMs on all the layers is to save computation time using parallelism; we will explain this in detail later.</p>

<p><img src="/public/img/reading/google-nmt-lstm-bidirectional.png" alt="" class="center" /></p>

<h5 id="residual-connections-across-layers">Residual Connections Across Layers</h5>

<p>Due to the gradient exploing / vanishing problems, simply stacking layers in deep nets are difficult to train. The residual connection targets this problem by transforming the output to the input plus a “residual”, which the network learns. In particular, if the input for layer <script type="math/tex">i</script> is <script type="math/tex">x_{i}</script>, then the residual tries to model <script type="math/tex">x_{i+1}</script> by</p>

<script type="math/tex; mode=display">x_{i+1} = x_{i} + f(x_i)</script>

<p>where <script type="math/tex">f(x_i)</script> contains the model parameters. This effectively solves the gradient explode / vanish problems because of the “shortcut connection” between the input and output.</p>

<p><img src="/public/img/reading/google-nmt-res.png" alt="" class="center" /></p>

<h4 id="decoder">Decoder</h4>

<p>The decoder is also an LSTM-RNN with 8 layers, where the input for each time step is the output for the previous time step (bottom layer) and the attention-weighted encoding vectors. The deocder does not have bidirectional LSTMs among the layers.</p>

<h3 id="optimization">Optimization</h3>

<p>Neural Machine Translation methods ususally have the following problems:</p>

<p><strong>Slow training and inference speed.</strong> This is naturally caused by the large model and large number of parameters. The model has multiple time step and multiple layers, so computing for even one sentence is very expensive.</p>

<p><strong>Lacks robustness in translating rare words.</strong> Rare words are either not translated or not translated correctly, since the words have low frequency in the training set, and probably have few sentences to train on, resulting to overfitting the limited amount of sentences.</p>

<p><strong>Fail to completely “cover” the input</strong>. Since there are not direct restrictions posed on completely covering the input, NMT has the tendency to do so.</p>

<p>Google addresses these following problems by the methods discussed below.</p>

<h4 id="data-paralleism-and-model-paralleism">Data Paralleism and Model Paralleism</h4>

<p>Using data parallelsim and model parallelism, the model training and inference speed can be effectively increased.</p>

<p>Data parallelism is straightfoward: the <script type="math/tex">n</script> model replicas are trained concurrently using a Downpour SGD algorithm.</p>

<p>Model parallelism takes advantage of the structure of the model - for deep LSTM with a single direction, the <script type="math/tex">i</script>-th layer can compute with the <script type="math/tex">(i+1)</script>th layer in parallel, with only a time step ahead. This would be most effective if the input sequence is very long.</p>

<p>However, model paralleism does not directly apply to the decoding structure, since reasonably one need to obtain the prediction at time step <script type="math/tex">t</script> to get the input of time step <script type="math/tex">t+1</script>. The Google people here adopted this hack - use the <strong>bottom layer</strong> of the decoder at step $$t, combined with the encoding vector with attention as the input for LSTM.</p>

<h4 id="quantizable-model-and-quantized-inference">Quantizable Model and Quantized Inference</h4>

<p>Quantizing the model allows for efficient inference, allowing for low latency translation.</p>

<p>To reduce quantization error, the accumulator values are restricted to be between <script type="math/tex">[-\delta, \delta]</script>, such that quantization can be utilized later.</p>

<p>The weight matrices <script type="math/tex">W</script> are represented as a 8-bit integer matrix <script type="math/tex">WQ</script>. The accumulator values are represented as 16-bit integers representing the range <script type="math/tex">[-\delta, \delta]</script>.</p>

<p>The softmax logits are restricted between the range <script type="math/tex">[-\gamma, \gamma]</script>.</p>

<p>The matrix multiplication are done using 8-bit integer arithmetic, and the summation are done using 16-bit integer arithmetic. Therefore, the quantized solution is quite efficient.</p>

<p>The quantized model does not have degraded performance compared to the original model. In fact the performance gets better, possibly due to the regularization imposed to the network.</p>

<h4 id="beam-search-for-maximum-likelihood">Beam Search For Maximum Likelihood</h4>

<p>Beam search is used to find the sequence <script type="math/tex">Y</script> that maximizes a score function <script type="math/tex">s(Y, X)</script> given a trained model. Two important refinements can be introduced.</p>

<p><strong>Length normalization</strong>: With length normalization, the hypothesis of different lengths are taken into account.</p>

<ul>
  <li>Normalize by length directly</li>
  <li>Normalize by length over the power of <script type="math/tex">\alpha</script>.</li>
  <li>Use a scoring function <script type="math/tex">s(Y, X)</script> with two parameters <script type="math/tex">\alpha</script> and <script type="math/tex">\beta</script>.</li>
</ul>

<p>There is a work from Facebook that tries to attach softmax. There method is called <a href="https://research.facebook.com/blog/building-an-efficient-neural-language-model-over-a-billion-words/">Adaptive Softmax</a>.</p>


</div>
</div>
<div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 col-sm-10 col-sm-offset-1">
        <hr style="margin-top: 50px; margin-bottom: 50px;" />

<div id="disqus_thread"></div>
<script>
    /**
     *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
     *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
     */
        //var disqus_developer = 1; // Comment out when the site is live
    var disqus_config = function () {
            this.page.url = "http://tsong.me/blog/google-nmt/";  // Replace PAGE_URL with your page's canonical URL variable
            this.page.identifier = "/blog/google-nmt/"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        };

    (function() {  // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');

        s.src = 'https://tsong-me.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
    </div>
</div>
</div>



<footer style="margin-top: 0px;">
    <div class="container" style="margin-top: 25px; margin-bottom: 20px;">
        <hr style="margin-top: 25px; margin-bottom: 25px;" />
        <p style="text-align: center; font-size: 14px;">
            © 2017 •
            <a href="">tsong.me</a> •
            <a href="" target="_top">tsong@cs.stanford.edu</a>
        </p>
    </div>
</footer>

<script src="//code.jquery.com/jquery-1.10.2.min.js"></script>
<script src="/public/js/bootstrap.min.js"></script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-68764449-3', 'auto');
    ga('send', 'pageview');

</script>
</body>
</html>
